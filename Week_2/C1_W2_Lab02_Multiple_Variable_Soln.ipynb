{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Lab: Multiple Variable Linear Regression\n",
    "\n",
    "In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review.\n",
    "# Outline\n",
    "- [&nbsp;&nbsp;1.1 Goals](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Tools](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notation](#toc_15456_1.3)\n",
    "- [2 Problem Statement](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matrix X containing our examples](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Parameter vector w, b](#toc_15456_2.2)\n",
    "- [3 Model Prediction With Multiple Variables](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 Single Prediction element by element](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Single Prediction, vector](#toc_15456_3.2)\n",
    "- [4 Compute Cost With Multiple Variables](#toc_15456_4)\n",
    "- [5 Gradient Descent With Multiple Variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Compute Gradient with Multiple Variables](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 Gradient Descent With Multiple Variables](#toc_15456_5.2)\n",
    "- [6 Congratulations](#toc_15456_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features\n",
    "    - Utilize NumPy `np.dot` to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Tools\n",
    "In this lab, we will make use of: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# 2 Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `X_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 Single Prediction element by element\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    print(f\"number: {n}\")\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        print(f\"x[{i}] : {x[i]}, w[{i}]: {w[i]}\")\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "number: 4\n",
      "x[0] : 2104, w[0]: 0.39133535\n",
      "x[1] : 5, w[1]: 18.75376741\n",
      "x[2] : 1, w[2]: -53.36032453\n",
      "x[3] : 45, w[3]: -26.42131618\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape of `x_vec`. It is a 1-D NumPy vector with 4 elements, (4,). The result, `f_wb` is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## 3.2 Single Prediction, vector\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.99999761940825\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1416    3    2   40]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and shapes are the same as the previous version which used looping. Going forward, `np.dot` will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904880036537e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**: Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "#     print(X.shape)\n",
    "    dj_dw = np.zeros((n,))\n",
    "#     print(dj_dw)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "#         print(f\"err: {err}\")\n",
    "        for j in range(n):\n",
    "#             print( f\"dj_dw[{j}]: {dj_dw[j]}, X[{i}, {j}] : {X[i, j]}\"  )\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j] \n",
    "#             print( f\"dj_dw[{j}]: {dj_dw[j]}\")\n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.673925169143331e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 Gradient Descent With Multiple Variables\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "        print(f\"w: {w}, b: {b}\")  \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you will test the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [2.41e-01 5.59e-04 1.84e-04 6.03e-03], b: 0.000145\n",
      "Iteration    0: Cost  2529.46   \n",
      "w: [1.95e-01 4.54e-04 1.34e-04 4.78e-03], b: 0.00011402564683333336\n",
      "w: [2.04e-01 4.79e-04 1.30e-04 4.94e-03], b: 0.00011714368908454539\n",
      "w: [2.02e-01 4.78e-04 1.17e-04 4.82e-03], b: 0.00011365700305146083\n",
      "w: [2.02e-01 4.82e-04 1.06e-04 4.75e-03], b: 0.00011145001540926669\n",
      "w: [2.02e-01 4.85e-04 9.45e-05 4.68e-03], b: 0.00010899524267421966\n",
      "w: [2.02e-01 4.88e-04 8.30e-05 4.61e-03], b: 0.00010658861070525292\n",
      "w: [2.02e-01 4.92e-04 7.15e-05 4.54e-03], b: 0.00010417278852237808\n",
      "w: [2.02e-01 4.95e-04 6.00e-05 4.46e-03], b: 0.00010175888309386385\n",
      "w: [2.02e-01 4.98e-04 4.86e-05 4.39e-03], b: 9.93447426134968e-05\n",
      "w: [2.02e-01 5.01e-04 3.71e-05 4.32e-03], b: 9.69307839537756e-05\n",
      "w: [2.02e-01 5.05e-04 2.56e-05 4.25e-03], b: 9.4516926346159e-05\n",
      "w: [2.02e-01 5.08e-04 1.41e-05 4.17e-03], b: 9.210318543221897e-05\n",
      "w: [2.02e-01 5.11e-04 2.59e-06 4.10e-03], b: 8.968955817558692e-05\n",
      "w: [ 2.02e-01  5.15e-04 -8.91e-06  4.03e-03], b: 8.727604515845119e-05\n",
      "w: [ 2.02e-01  5.18e-04 -2.04e-05  3.96e-03], b: 8.486264626196219e-05\n",
      "w: [ 2.02e-01  5.21e-04 -3.19e-05  3.88e-03], b: 8.244936150308549e-05\n",
      "w: [ 2.02e-01  5.25e-04 -4.34e-05  3.81e-03], b: 8.003619087247492e-05\n",
      "w: [ 2.02e-01  5.28e-04 -5.49e-05  3.74e-03], b: 7.762313436588211e-05\n",
      "w: [ 2.02e-01  5.31e-04 -6.64e-05  3.67e-03], b: 7.52101919780713e-05\n",
      "w: [ 2.02e-01  5.34e-04 -7.79e-05  3.59e-03], b: 7.279736370399828e-05\n",
      "w: [ 2.02e-01  5.38e-04 -8.93e-05  3.52e-03], b: 7.038464953858204e-05\n",
      "w: [ 2.02e-01  5.41e-04 -1.01e-04  3.45e-03], b: 6.797204947674893e-05\n",
      "w: [ 2.02e-01  5.44e-04 -1.12e-04  3.38e-03], b: 6.555956351342422e-05\n",
      "w: [ 2.02e-01  5.48e-04 -1.24e-04  3.30e-03], b: 6.314719164353353e-05\n",
      "w: [ 2.02e-01  5.51e-04 -1.35e-04  3.23e-03], b: 6.0734933862002795e-05\n",
      "w: [ 2.02e-01  5.54e-04 -1.47e-04  3.16e-03], b: 5.832279016375809e-05\n",
      "w: [ 2.02e-01  5.58e-04 -1.58e-04  3.09e-03], b: 5.591076054372577e-05\n",
      "w: [ 2.02e-01  5.61e-04 -1.70e-04  3.01e-03], b: 5.349884499683239e-05\n",
      "w: [ 2.02e-01  5.64e-04 -1.81e-04  2.94e-03], b: 5.1087043518004745e-05\n",
      "w: [ 2.02e-01  5.67e-04 -1.93e-04  2.87e-03], b: 4.867535610216984e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 4.6263782744254945e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 4.3852323439187454e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 4.1440978181895106e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 3.90297469673058e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 3.661862979034771e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 3.4207626645949134e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 3.1796737529038755e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 2.9385962434545333e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 2.6975301357397904e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 2.4564754292525745e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 2.2154321234858356e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 1.9744002179325486e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 1.7333797120857024e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 1.4923706054383166e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 1.25137289748343e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 1.0103865877141087e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: 7.69411675623432e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: 5.284481607045098e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: 2.874960424504705e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: 4.655532035446473e-07\n",
      "w: [ 0.2  0.  -0.   0. ], b: -1.9437400609033058e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: -4.352919373907152e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: -6.761984740534744e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: -9.17093616585359e-06\n",
      "w: [ 0.2  0.  -0.   0. ], b: -1.1579773654931155e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -1.3988497212834475e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -1.6397106844630466e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -1.880560255538579e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -2.1213984350166903e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -2.3622252234040083e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -2.6030406212071226e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -2.8438446289326215e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -3.084637247087055e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -3.3254184761769554e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -3.5661883167088306e-05\n",
      "w: [ 0.2  0.  -0.   0. ], b: -3.8069467691891716e-05\n",
      "w: [ 2.02e-01  6.89e-04 -6.18e-04  1.89e-04], b: -4.047693834124443e-05\n",
      "w: [ 2.02e-01  6.93e-04 -6.29e-04  1.16e-04], b: -4.288429512021081e-05\n",
      "w: [ 2.02e-01  6.96e-04 -6.41e-04  4.39e-05], b: -4.5291538033855116e-05\n",
      "w: [ 2.02e-01  6.99e-04 -6.52e-04 -2.84e-05], b: -4.7698667087241305e-05\n",
      "w: [ 2.02e-01  7.02e-04 -6.64e-04 -1.01e-04], b: -5.010568228543311e-05\n",
      "w: [ 2.02e-01  7.06e-04 -6.75e-04 -1.73e-04], b: -5.251258363349406e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -5.491937113648748e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -5.7326044799476384e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -5.973260462752367e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -6.213905062569193e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -6.454538279904355e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -6.695160115264073e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -6.935770569154545e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -7.176369642081931e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -7.41695733455239e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -7.657533647072051e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -7.89809858014701e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -8.138652134283358e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -8.379194309987148e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -8.619725107764419e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -8.860244528121185e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -9.100752571563439e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -9.341249238597149e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -9.581734529728265e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -9.822208445462711e-05\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00010062670986306389\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00010303122152765174\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.0001054356194534493\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00010783990364551486\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00011024407410890655\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00011264813084868231\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00011505207386989976\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00011745590317761636\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00011985961877688931\n",
      "Iteration  100: Cost   695.99   \n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00012226322067277568\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.0001246667088703322\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00012707008337461533\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00012947334419068156\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00013187649132358683\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00013427952477838712\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.000136682444560138\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00013908525067389497\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00014148794312471316\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00014389052191764756\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00014629298705775293\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.0001486953385500838\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00015109757639969443\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00015349970061163894\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00015590171119097117\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00015830360814274473\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00016070539147201304\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00016310706118382923\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00016550861728324628\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00016791005977531692\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00017031138866509366\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00017271260395762872\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00017511370565797424\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00017751469377118198\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.0001799155683023036\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00018231632925639038\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00018471697663849357\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00018711751045366403\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00018951793070695253\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00019191823740340947\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00019431843054808519\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00019671851014602964\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00019911847620229266\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00020151832872192387\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00020391806770997257\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.0002063176931714879\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00020871720511151875\n",
      "w: [ 0.2  0.  -0.  -0. ], b: -0.00021111660353511388\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00021351588844732174\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00021591505985319045\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00021831411775776813\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00022071306216610252\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002231118930832412\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00022551061051423146\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00022790921446412047\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00023030770493795506\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00023270608194078194\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00023510434547764755\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00023750249555359808\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002399005321736795\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00024229845534293765\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000244696265066418\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002470939613491659\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002494915441962264\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002518890136126444\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002542863696034646\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002566836121737313\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002590807413284888\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000261477757072781\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002638746594116517\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00026627144835014434\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002686681238933023\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002710646860461686\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00027346113481378613\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002758574702011975\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002782536922134451\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00028064980085557115\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002830457961326175\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000285441678049626\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002878374466116381\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002902331018236951\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00029262864369083795\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0002950240722181076\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00029741938741054466\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00029981458927318946\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003022096778110822\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00030460465302926276\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003069995149327709\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00030939426352664614\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00031178889881592763\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003141834208056545\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00031657782950086547\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003189721249065992\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003213663070278941\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00032376037586978823\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003261543314373196\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00032854817373552576\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003309419027694443\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003333355185441124\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003357290210645672\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003381224103358453\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00034051568636298343\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003429088491510179\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00034530189870498484\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003476948350299201\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00035008765813085945\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003524803680128383\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00035487296468089186\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003572654481400551\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003596578183953629\n",
      "Iteration  200: Cost   694.92   \n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003620500754518498\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00036444221931455006\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00036683424998849786\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00036922616747872707\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00037161797179027133\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00037400966292816415\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00037640124089743866\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00037879270570312796\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003811840573502647\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003835752958438815\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00038596642118901065\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00038835743339068427\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003907483324539342\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003931391183837921\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003955297911852894\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0003979203508634573\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004003107974233268\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00040270113086992855\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00040509135120829324\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004074814584434511\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00040987145258043224\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004122613336242665\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00041465110157998345\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004170407564526126\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004194302982471831\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004218197269687239\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004242090426222638\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00042659824521283125\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004289873347454546\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004313763112251619\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00043376517465698095\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004361539250459394\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004385425623970647\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000440931086715384\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004433194980059242\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004457077962737121\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004480959815237742\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00045048405376113673\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004528720129908258\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004552598592178672\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00045764759244728664\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00046003521268410944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004624227199333607\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004648101142000655\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004671973954892484\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000469584563805934\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00047197161915514654\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004743585615419101\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00047674539097124853\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00047913210744818534\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00048151871097774397\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00048390520156494754\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000486291579214819\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000488677843932381\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000491063995722656\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004934500345906665\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004958359605414343\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0004982217735799813\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005006074737113291\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.000502993060940499\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005053785352725123\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005077638967123898\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005101491452651522\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.00051253428093582\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005149193037294134\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005173042136509525\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005196890107054571\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005220736948979467\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005244582662334407\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005268427247169582\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005292270703535182\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005316113031481394\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005339954231058402\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005363794302316389\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005387633245305534\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005411471060076016\n",
      "w: [ 0.2   0.   -0.   -0.01], b: -0.0005435307746678011\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005459143305161691\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005482977735577229\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005506811037974793\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005530643212404551\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005554474258916665\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.00055783041775613\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005602132968388615\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005625960631448768\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005649787166791914\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005673612574468208\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.00056974368545278\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000572126000702084\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005745082031997473\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005768902929507845\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005792722699602098\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005816541342330371\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005840358857742804\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005864175245889531\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005887990506820686\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005911804640586399\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005935617647236799\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005959429526822015\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0005983240279392168\n",
      "Iteration  300: Cost   693.86   \n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006007049904997383\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006030858403687779\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006054665775513473\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006078472020524581\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006102277138771217\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006126081130303491\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006149883995171514\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000617368573342539\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006197486345115225\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000622128583029112\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006245084189003175\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006268881421301489\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006292677527236157\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006316472506857272\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006340266360214925\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006364059087359204\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006387850688340198\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006411641163207989\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006435430512012659\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006459218734804289\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006483005831632956\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006506791802548737\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006530576647601703\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006554360366841926\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006578142960319475\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006601924428084416\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006625704770186813\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006649483986676729\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006673262077604224\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006697039043019354\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006720814882972177\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006744589597512744\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006768363186691106\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006792135650557313\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000681590698916141\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006839677202553443\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006863446290783452\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006887214253901478\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006910981091957559\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006934746805001729\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006958511393084024\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0006982274856254471\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007006037194563102\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007029798408059941\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007053558496795014\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007077317460818342\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007101075300179945\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007124832014929842\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007148587605118045\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007172342070794571\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007196095412009429\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007219847628812627\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007243598721254172\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007267348689384068\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007291097533252319\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007314845252908922\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007338591848403876\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007362337319787176\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007386081667108816\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007409824890418786\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007433566989767074\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007457307965203667\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000748104781677855\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007504786544541704\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007528524148543109\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007552260628832743\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007575995985460582\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007599730218476597\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007623463327930761\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007647195313873041\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007670926176353405\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007694655915421816\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007718384531128236\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007742112023522625\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000776583839265494\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007789563638575138\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007813287761333171\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000783701076097899\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007860732637562543\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007884453391133778\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007908173021742639\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007931891529439066\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0007955608914273001\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000797932517629438\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000800304031555314\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008026754332099213\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000805046722598253\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008074178997253019\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008097889645960608\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008121599172155221\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008145307575886779\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008169014857205202\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008192721016160408\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008216426052802312\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008240129967180828\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008263832759345866\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008287534429347335\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008311234977235141\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008334934403059189\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008358632706869382\n",
      "Iteration  400: Cost   692.81   \n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008382329888715618\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008406025948647796\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.000842972088671581\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008453414702969555\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008477107397458919\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008500798970233793\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008524489421344063\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008548178750839614\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008571866958770327\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008595554045186082\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008619240010136757\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008642924853672228\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008666608575842367\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008690291176697045\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008713972656286133\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008737653014659494\n",
      "w: [ 0.2   0.   -0.   -0.02], b: -0.0008761332251866996\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008785010367958499\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008808687362983862\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008832363236992945\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008856037990035603\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008879711622161688\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008903384133421051\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008927055523863541\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008950725793539007\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.000897439494249729\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0008998062970788234\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.000902172987846168\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009045395665567463\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.000906906033215542\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009092723878275385\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009116386303977187\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009140047609310657\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.000916370779432562\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009187366859071902\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009211024803599324\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009234681627957708\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009258337332196869\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009281991916366625\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009305645380516787\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009329297724697168\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009352948948957576\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009376599053347818\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009400248037917699\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009423895902717021\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009447542647795584\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009471188273203185\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009494832778989621\n",
      "w: [ 0.2   0.   -0.   -0.03], b: -0.0009518476165204685\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009542118431898167\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009565759579119858\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009589399606919544\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009613038515347009\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009636676304452036\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009660312974284405\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009683948524893894\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009707582956330279\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009731216268643333\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009754848461882827\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009778479536098532\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009802109491340212\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009825738327657633\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009849366045100558\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009872992643718747\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009896618123561957\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009920242484679946\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009943865727122466\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.000996748785093927\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0009991108856180102\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010014728742894715\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010038347511132852\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010061965160944254\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010085581692378662\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010109197105485814\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010132811400315447\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010156424576917293\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010180036635341087\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010203647575636554\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010227257397853423\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001025086610204142\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010274473688250264\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001029808015652968\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010321685506929383\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010345289739499088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010368892854288514\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010392494851347368\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001041609573072536\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010439695492472198\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010463294136637588\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010486891663271229\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010510488072422825\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010534083364142073\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001055767753847867\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010581270595482308\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010604862535202682\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010628453357689478\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010652043062992385\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010675631651161088\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001069921912224527\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010722805476294612\n",
      "Iteration  500: Cost   691.77   \n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001074639071335879\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010769974833487483\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010793557836730363\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010817139723137104\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010840720492757375\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010864300145640843\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001088787868183717\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010911456101396024\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010935032404367064\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010958607590799948\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0010982181660744333\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001100575461424987\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011029326451366217\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011052897172143018\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011076466776629925\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001110003526487658\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011123602636932627\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011147168892847708\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001117073403267146\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001119429805645352\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011217860964243523\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.00112414227560911\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011264983432045883\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011288542992157497\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011312101436475569\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011335658765049721\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011359214977929575\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001138277007516475\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001140632405680486\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011429876922899526\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011453428673498353\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011476979308650955\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001150052882840694\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011524077232815911\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011547624521927474\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011571170695791227\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001159471575445677\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011618259697973702\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011641802526391617\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011665344239760104\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011688884838128757\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011712424321547164\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011735962690064908\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011759499943731573\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011783036082596743\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011806571106709994\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011830105016120904\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001185363781087905\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011877169491034\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011900700056635327\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011924229507732599\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001194775784437538\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011971285066613238\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0011994811174495731\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001201833616807242\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.001204186004739286\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0012065382812506608\n",
      "w: [ 0.2   0.   -0.01 -0.03], b: -0.0012088904463463215\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012112425000312232\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012135944423103209\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012159462731885689\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012182979926709217\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012206496007623336\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012230010974677585\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00122535248279215\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012277037567404619\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012300549193176471\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001232405970528659\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012347569103784502\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012371077388719732\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012394584560141808\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001241809061810025\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012441595562644576\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012465099393824306\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012488602111688954\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012512103716288032\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012535604207671053\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012559103585887525\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012582601850986955\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012606099003018845\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00126295950420327\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012653089968078016\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012676583781204293\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012700076481461028\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012723568068897712\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012747058543563837\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001277054790550889\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012794036154782358\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012817523291433726\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012841009315512479\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012864494227068093\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012887978026150048\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012911460712807818\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012934942287090878\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0012958422749048699\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001298190209873075\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013005380336186495\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00130288574614654\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013052333474616931\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013075808375690545\n",
      "Iteration  600: Cost   690.73   \n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00130992821647357\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013122754841801854\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001314622640693846\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013169696860194967\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013193166201620826\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013216634431265483\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013240101549178384\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001326356755540897\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013287032450006685\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013310496233020963\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013333958904501242\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013357420464496957\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013380880913057536\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013404340250232412\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013427798476071012\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013451255590622758\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013474711593937077\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013498166486063386\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013521620267051104\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013545072936949649\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013568524495808434\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013591974943676871\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001361542428060437\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013638872506640338\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001366231962183418\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00136857656262353\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.00137092105198931\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013732654302856975\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013756096975176325\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001377953853690054\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013802978988079018\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013826418328761144\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013849856558996309\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013873293678833896\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001389672968832329\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001392016458751387\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013943598376455015\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0013967031055196104\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001399046262378651\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014013893082275607\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014037322430712764\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001406075066914735\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014084177797628727\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014107603816206262\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014131028724929315\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014154452523847246\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014177875213009412\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014201296792465167\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014224717262263863\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014248136622454853\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014271554873087483\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014294972014211099\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014318388045875044\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014341802968128661\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001436521678102129\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014388629484602268\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014412041078920928\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014435451564026604\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014458860939968628\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014482269206796325\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014505676364559026\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014529082413306052\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014552487353086724\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014575891183950364\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001459929390594629\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014622695519123812\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001464609602353225\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014669495419220911\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014692893706239104\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014716290884636135\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001473968695446131\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001476308191576393\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014786475768593294\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014809868512998703\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001483326014902945\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014856650676734829\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001488004009616413\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014903428407366643\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014926815610391655\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001495020170528845\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014973586692106312\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0014996970570894518\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001502035334170235\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001504373500457908\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015067115559573984\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015090495006736331\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015113873346115393\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015137250577760438\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015160626701720727\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015184001718045524\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015207375626784092\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015230748427985687\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015254120121699565\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.001527749070797498\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015300860186861184\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015324228558407425\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015347595822662954\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015370961979677014\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015394327029498848\n",
      "w: [ 0.2   0.   -0.01 -0.04], b: -0.0015417690972177696\n",
      "Iteration  700: Cost   689.71   \n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015441053807762797\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015464415536303388\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015487776157848702\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015511135672447974\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015534494080150431\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015557851381005302\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015581207575061811\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015604562662369181\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015627916642976635\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015651269516933392\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015674621284288667\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015697971945091674\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015721321499391628\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015744669947237737\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001576801728867921\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015791363523765252\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015814708652545067\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015838052675067856\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015861395591382818\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015884737401539151\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001590807810558605\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0015931417703572706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001595475619554831\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001597809358156205\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016001429861663113\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016024765035900683\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016048099104323942\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016071432066982066\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016094763923924238\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016118094675199627\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001614142432085741\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016164752860946758\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016188080295516836\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016211406624616813\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016234731848295853\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016258055966603118\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016281378979587768\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001630470088729896\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001632802168978585\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016351341387097593\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016374659979283336\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001639797746639223\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016421293848473421\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016444609125576056\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016467923297749274\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016491236365042217\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016514548327504024\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016537859185183827\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016561168938130763\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016584477586393963\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016607785130022556\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016631091569065668\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016654396903572424\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016677701133591947\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016701004259173358\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016724306280365775\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016747607197218316\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016770907009780091\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016794205718100215\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016817503322227795\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016840799822211941\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016864095218101756\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016887389509946343\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016910682697794807\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001693397478169624\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0016957265761699744\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001698055563785441\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017003844410209332\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.00170271320788136\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017050418643716298\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017073704104966518\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001709698846261334\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017120271716705843\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017143553867293108\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017166834914424211\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001719011485814823\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017213393698514233\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017236671435571292\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017259948069368476\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001728322359995485\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017306498027379478\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017329771351691423\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017353043572939742\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017376314691173492\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001739958470644173\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017422853618793506\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017446121428277873\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001746938813494388\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017492653738840572\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017515918240016994\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017539181638522186\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001756244393440519\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017585705127715042\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017608965218500778\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001763222420681143\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001765548209269603\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017678738876203608\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001770199455738319\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.00177252491362838\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001774850261295446\n",
      "Iteration  800: Cost   688.70   \n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001777175498744419\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017795006259802009\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017818256430076932\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017841505498317973\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017864753464574142\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001788800032889445\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017911246091327903\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017934490751923505\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001795773431073026\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0017980976767797171\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018004218123173234\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018027458376907444\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018050697529048794\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018073935579646279\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018097172528748885\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018120408376405604\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018143643122665418\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001816687676757731\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018190109311190263\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018213340753553255\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001823657109471526\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018259800334725256\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018283028473632214\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018306255511485103\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001832948144833289\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018352706284224544\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018375930019209027\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018399152653335298\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018422374186652319\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018445594619209045\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018468813951054432\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018492032182237433\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018515249312807\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018538465342812075\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018561680272301609\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018584894101324545\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018608106829929826\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018631318458166388\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018654528986083172\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001867773841372911\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018700946741153138\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.0018724153968404185\n",
      "w: [ 0.2   0.   -0.01 -0.05], b: -0.001874736009553118\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001877056512258305\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018793769049608717\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018816971876657108\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018840173603777138\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018863374231017727\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001888657375842779\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018909772186056241\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001893296951395199\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0018956165742163948\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001897936087074102\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019002554899732113\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019025747829186127\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019048939659151963\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001907213038967852\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019095320020814694\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001911850855260938\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019141695985111465\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019164882318369843\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.00191880675524334\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019211251687351022\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001923443472317159\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019257616659943987\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019280797497717092\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019303977236539778\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019327155876460922\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019350333417529394\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019373509859794066\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019396685203303806\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019419859448107479\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019443032594253949\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019466204641792075\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019489375590770717\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019512545441238735\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019535714193244977\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.00195588818468383\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019582048402067556\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001960521385898159\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019628378217629253\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019651541478059383\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019674703640320825\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019697864704462417\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019721024670532996\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019744183538581398\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019767341308656455\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019790497980807\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019813653555081864\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019836808031529866\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019859961410199833\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001988311369114059\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019906264874400958\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001992941496002975\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019952563948075785\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.001997571183858787\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0019998858631614827\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002002200432720546\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020045148925408574\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020068292426272975\n",
      "Iteration  900: Cost   687.69   \n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020091434829847465\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020114576136180846\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020137716345321917\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020160855457319474\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020183993472222307\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020207130390079214\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002023026621093898\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002025340093485039\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020276534561862237\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.00202996670920233\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002032279852538236\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002034592886198819\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002036905810188958\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002039218624513529\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020415313291774107\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002043843924185479\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020461564095426107\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020484687852536827\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020507810513235715\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002053093207757153\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002055405254559303\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020577171917348976\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002060029019288812\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020623407372259215\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020646523455511013\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020669638442692264\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020692752333851708\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020715865129038095\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020738976828300164\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020762087431686655\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020785196939246305\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002080830535102785\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020831412667080026\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020854518887451563\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020877624012191186\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020900728041347624\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020923830974969603\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020946932813105845\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002097003355580507\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0020993133203115995\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021016231755087334\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021039329211767805\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021062425573206115\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002108552083945098\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.00211086150105511\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021131708086555186\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021154800067511937\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021177890953470056\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021200980744478237\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021224069440585183\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021247157041839587\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021270243548290138\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021293328959985525\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002131641327697444\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021339496499305564\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021362578627027585\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021385659660189183\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021408739598839034\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002143181844302582\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002145489619279821\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002147797284820488\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021501048409294497\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021524122876115733\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021547196248717254\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021570268527147723\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.00215933397114558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002161640980169015\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021639478797899426\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021662546700132283\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021685613508437376\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021708679222863353\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021731743843458867\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002175480737027256\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021777869803353083\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021800931142749074\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002182399138850917\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021847050540682016\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021870108599316243\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021893165564460487\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002191622143616338\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021939276214473545\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021962329899439617\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0021985382491110216\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.002200843398953397\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0022031484394759494\n",
      "w: [ 0.2   0.   -0.01 -0.06], b: -0.0022054533706835407\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.002207758192581033\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.002210062905173287\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022123675084651647\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022146720024615266\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.002216976387167234\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022192806625871467\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022215848287261255\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022238888855890303\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.002226192833180721\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022284966715060577\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022308004005698995\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.0022331040203771055\n",
      "w: [ 0.2   0.   -0.01 -0.07], b: -0.002235407530932535\n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"toc_15456_6\"></a>\n",
    "# 6 Congratulations!\n",
    "In this lab you:\n",
    "- Redeveloped the routines for linear regression, now with multiple variables.\n",
    "- Utilized NumPy `np.dot` to vectorize the implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
